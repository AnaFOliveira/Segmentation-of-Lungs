{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting generator_testing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile generator_testing.py\n",
    "\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "\n",
    "from .utils import pickle_dump, pickle_load\n",
    "from .utils.patches import compute_patch_indices, get_random_nd_index, get_patch_from_3d_data\n",
    "from .augment import augment_data, random_permutation_x_y\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_validation_generators(data_file, batch_size, n_labels, validation_keys_file,\n",
    "                                           overwrite=False, labels=None, augment=False,\n",
    "                                           augment_flip=True, augment_distortion_factor=0.25, patch_shape=None,\n",
    "                                           validation_patch_overlap=0, validation_batch_size=None, skip_blank=True, \n",
    "                                           permute=False):\n",
    "    \"\"\"\n",
    "    Creates the validation generators that can be used when training the model.\n",
    "    :param skip_blank: If True, any blank (all-zero) label images/patches will be skipped by the data generator.\n",
    "    :param validation_batch_size: Batch size for the validation data.\n",
    "    :param training_patch_start_offset: Tuple of length 3 containing integer values. Training data will randomly be\n",
    "    offset by a number of pixels between (0, 0, 0) and the given tuple. (default is None)\n",
    "    :param validation_patch_overlap: Number of pixels/voxels that will be overlapped in the validation data. (requires\n",
    "    patch_shape to not be None)\n",
    "    :param patch_shape: Shape of the data to return with the generator. If None, the whole image will be returned.\n",
    "    (default is None)\n",
    "    :param augment_flip: if True and augment is True, then the data will be randomly flipped along the x, y and z axis\n",
    "    :param augment_distortion_factor: if augment is True, this determines the standard deviation from the original\n",
    "    that the data will be distorted (in a stretching or shrinking fashion). Set to None, False, or 0 to prevent the\n",
    "    augmentation from distorting the data in this way.\n",
    "    :param augment: If True, training data will be distorted on the fly so as to avoid over-fitting.\n",
    "    :param labels: List or tuple containing the ordered label values in the image files. The length of the list or tuple\n",
    "    should be equal to the n_labels value.\n",
    "    Example: (10, 25, 50)\n",
    "    The data generator would then return binary truth arrays representing the labels 10, 25, and 30 in that order.\n",
    "    :param data_file: hdf5 file to load the data from.\n",
    "    :param batch_size: Size of the batches that the training generator will provide.\n",
    "    :param n_labels: Number of binary labels.\n",
    "    :param training_keys_file: Pickle file where the index locations of the training data will be stored.\n",
    "    :param validation_keys_file: Pickle file where the index locations of the validation data will be stored.\n",
    "    :param data_split: How the training and validation data will be split. 0 means all the data will be used for\n",
    "    validation and none of it will be used for training. 1 means that all the data will be used for training and none\n",
    "    will be used for validation. Default is 0.8 or 80%.\n",
    "    :param overwrite: If set to True, previous files will be overwritten. The default mode is false, so that the\n",
    "    training and validation splits won't be overwritten when rerunning model training.\n",
    "    :param permute: will randomly permute the data (data must be 3D cube)\n",
    "    :return: Training data generator, validation data generator, number of training steps, number of validation steps\n",
    "    \"\"\"\n",
    "    if not validation_batch_size:\n",
    "        validation_batch_size = batch_size\n",
    "\n",
    "    validation_list = get_validation_split(data_file,\n",
    "                                                          data_split=data_split,\n",
    "                                                          overwrite=overwrite,\n",
    "                                                          validation_file=validation_keys_file)\n",
    "\n",
    "    validation_generator = data_generator(data_file, validation_list,\n",
    "                                          batch_size=validation_batch_size,\n",
    "                                          n_labels=n_labels,\n",
    "                                          labels=labels,\n",
    "                                          patch_shape=patch_shape,\n",
    "                                          patch_overlap=validation_patch_overlap,\n",
    "                                          skip_blank=skip_blank)\n",
    "\n",
    "    \n",
    "    num_validation_steps = get_number_of_steps(get_number_of_patches(data_file, validation_list, patch_shape,\n",
    "                                                                     skip_blank=skip_blank,\n",
    "                                                                     patch_overlap=validation_patch_overlap),\n",
    "                                               validation_batch_size)\n",
    "    print(\"Number of validation steps: \", num_validation_steps)\n",
    "\n",
    "    return validation_generator, num_validation_steps\n",
    "\n",
    "\n",
    "def get_number_of_steps(n_samples, batch_size):\n",
    "    if n_samples <= batch_size:\n",
    "        return n_samples\n",
    "    elif np.remainder(n_samples, batch_size) == 0:\n",
    "        return n_samples//batch_size\n",
    "    else:\n",
    "        return n_samples//batch_size + 1\n",
    "\n",
    "\n",
    "def get_validation_split(data_file, validation_file, data_split=0.8, overwrite=False):\n",
    "    \"\"\"\n",
    "    Splits the data into the training and validation indices list.\n",
    "    :param data_file: pytables hdf5 data file\n",
    "    :param training_file:\n",
    "    :param validation_file:\n",
    "    :param data_split:\n",
    "    :param overwrite:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if overwrite or not os.path.exists(training_file):\n",
    "        print(\"Creating validation split...\")\n",
    "        nb_samples = data_file.root.data.shape[0]\n",
    "        sample_list = list(range(nb_samples))\n",
    "        validation_list= sample_list\n",
    "        #training_list, validation_list = split_list(sample_list, split=data_split)\n",
    "        pickle_dump(validation_list, validation_file)\n",
    "        return validation_list\n",
    "    else:\n",
    "        print(\"Loading previous validation split...\")\n",
    "        return pickle_load(validation_file)\n",
    "\n",
    "\n",
    "def split_list(input_list, split=0.8, shuffle_list=True):\n",
    "    if shuffle_list:\n",
    "        shuffle(input_list)\n",
    "    n_training = int(len(input_list) * split)\n",
    "    training = input_list[:n_training]\n",
    "    testing = input_list[n_training:]\n",
    "    return training, testing\n",
    "\n",
    "\n",
    "def data_generator(data_file, index_list, batch_size=1, n_labels=1, labels=None, augment=False, augment_flip=True,\n",
    "                   augment_distortion_factor=0.25, patch_shape=None, patch_overlap=0, patch_start_offset=None,\n",
    "                   shuffle_index_list=True, skip_blank=True, permute=False):\n",
    "    orig_index_list = index_list\n",
    "    while True:\n",
    "        x_list = list()\n",
    "        y_list = list()\n",
    "        if patch_shape:\n",
    "            index_list = create_patch_index_list(orig_index_list, data_file.root.data.shape[-3:], patch_shape,\n",
    "                                                 patch_overlap, patch_start_offset)\n",
    "        else:\n",
    "            index_list = copy.copy(orig_index_list)\n",
    "\n",
    "        if shuffle_index_list:\n",
    "            shuffle(index_list)\n",
    "        while len(index_list) > 0:\n",
    "            index = index_list.pop()\n",
    "            add_data(x_list, y_list, data_file, index, augment=augment, augment_flip=augment_flip,\n",
    "                     augment_distortion_factor=augment_distortion_factor, patch_shape=patch_shape,\n",
    "                     skip_blank=skip_blank, permute=permute)\n",
    "            if len(x_list) == batch_size or (len(index_list) == 0 and len(x_list) > 0):\n",
    "                yield convert_data(x_list, y_list, n_labels=n_labels, labels=labels)\n",
    "                x_list = list()\n",
    "                y_list = list()\n",
    "\n",
    "\n",
    "def get_number_of_patches(data_file, index_list, patch_shape=None, patch_overlap=0, patch_start_offset=None,\n",
    "                          skip_blank=True):\n",
    "    if patch_shape:\n",
    "        index_list = create_patch_index_list(index_list, data_file.root.data.shape[-3:], patch_shape, patch_overlap,\n",
    "                                             patch_start_offset)\n",
    "        count = 0\n",
    "        for index in index_list:\n",
    "            x_list = list()\n",
    "            y_list = list()\n",
    "            add_data(x_list, y_list, data_file, index, skip_blank=skip_blank, patch_shape=patch_shape)\n",
    "            if len(x_list) > 0:\n",
    "                count += 1\n",
    "        return count\n",
    "    else:\n",
    "        return len(index_list)\n",
    "\n",
    "\n",
    "def create_patch_index_list(index_list, image_shape, patch_shape, patch_overlap, patch_start_offset=None):\n",
    "    patch_index = list()\n",
    "    for index in index_list:\n",
    "        if patch_start_offset is not None:\n",
    "            random_start_offset = np.negative(get_random_nd_index(patch_start_offset))\n",
    "            patches = compute_patch_indices(image_shape, patch_shape, overlap=patch_overlap, start=random_start_offset)\n",
    "        else:\n",
    "            patches = compute_patch_indices(image_shape, patch_shape, overlap=patch_overlap)\n",
    "        patch_index.extend(itertools.product([index], patches))\n",
    "    return patch_index\n",
    "\n",
    "\n",
    "def add_data(x_list, y_list, data_file, index, augment=False, augment_flip=False, augment_distortion_factor=0.25,\n",
    "             patch_shape=False, skip_blank=True, permute=False):\n",
    "    \"\"\"\n",
    "    Adds data from the data file to the given lists of feature and target data\n",
    "    :param skip_blank: Data will not be added if the truth vector is all zeros (default is True).\n",
    "    :param patch_shape: Shape of the patch to add to the data lists. If None, the whole image will be added.\n",
    "    :param x_list: list of data to which data from the data_file will be appended.\n",
    "    :param y_list: list of data to which the target data from the data_file will be appended.\n",
    "    :param data_file: hdf5 data file.\n",
    "    :param index: index of the data file from which to extract the data.\n",
    "    :param augment: if True, data will be augmented according to the other augmentation parameters (augment_flip and\n",
    "    augment_distortion_factor)\n",
    "    :param augment_flip: if True and augment is True, then the data will be randomly flipped along the x, y and z axis\n",
    "    :param augment_distortion_factor: if augment is True, this determines the standard deviation from the original\n",
    "    that the data will be distorted (in a stretching or shrinking fashion). Set to None, False, or 0 to prevent the\n",
    "    augmentation from distorting the data in this way.\n",
    "    :param permute: will randomly permute the data (data must be 3D cube)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data, truth = get_data_from_file(data_file, index, patch_shape=patch_shape)\n",
    "    if augment:\n",
    "        if patch_shape is not None:\n",
    "            affine = data_file.root.affine[index[0]]\n",
    "        else:\n",
    "            affine = data_file.root.affine[index]\n",
    "        data, truth = augment_data(data, truth, affine, flip=augment_flip, scale_deviation=augment_distortion_factor)\n",
    "\n",
    "    if permute:\n",
    "        if data.shape[-3] != data.shape[-2] or data.shape[-2] != data.shape[-1]:\n",
    "            raise ValueError(\"To utilize permutations, data array must be in 3D cube shape with all dimensions having \"\n",
    "                             \"the same length.\")\n",
    "        data, truth = random_permutation_x_y(data, truth[np.newaxis])\n",
    "    else:\n",
    "        truth = truth[np.newaxis]\n",
    "\n",
    "    if not skip_blank or np.any(truth != 0):\n",
    "        x_list.append(data)\n",
    "        y_list.append(truth)\n",
    "\n",
    "\n",
    "def get_data_from_file(data_file, index, patch_shape=None):\n",
    "    if patch_shape:\n",
    "        index, patch_index = index\n",
    "        data, truth = get_data_from_file(data_file, index, patch_shape=None)\n",
    "        x = get_patch_from_3d_data(data, patch_shape, patch_index)\n",
    "        y = get_patch_from_3d_data(truth, patch_shape, patch_index)\n",
    "    else:\n",
    "        x, y = data_file.root.data[index], data_file.root.truth[index, 0]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def convert_data(x_list, y_list, n_labels=1, labels=None):\n",
    "    x = np.asarray(x_list)\n",
    "    y = np.asarray(y_list)\n",
    "    if n_labels == 1:\n",
    "        y[y > 0] = 1\n",
    "    elif n_labels > 1:\n",
    "        y = get_multi_class_labels(y, n_labels=n_labels, labels=labels)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_multi_class_labels(data, n_labels, labels=None):\n",
    "    \"\"\"\n",
    "    Translates a label map into a set of binary labels.\n",
    "    :param data: numpy array containing the label map with shape: (n_samples, 1, ...).\n",
    "    :param n_labels: number of labels.\n",
    "    :param labels: integer values of the labels.\n",
    "    :return: binary numpy array of shape: (n_samples, n_labels, ...)\n",
    "    \"\"\"\n",
    "    new_shape = [data.shape[0], n_labels] + list(data.shape[2:])\n",
    "    y = np.zeros(new_shape, np.int8)\n",
    "    for label_index in range(n_labels):\n",
    "        if labels is not None:\n",
    "            y[:, label_index][data[:, 0] == labels[label_index]] = 1\n",
    "        else:\n",
    "            y[:, label_index][data[:, 0] == (label_index + 1)] = 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
